 In this chapter we introduce the structure of the WSC and the  
 %TODO add good opening sentence here
 In section 2.1 we introduce the features of the WSC as proposed by Levesque et al. \cite{DBLP:conf/kr/LevesqueDM12}. Additionally we explain the dataset of Pronoun Disambiguation Problems (PDPS) which was proposed by \cite{DBLP:conf/aaai/MorgensternO15}. Finally, we discuss what makes the WSC so hard for solving and what are the main limitations of this challenge. In section 2.2 we analyze the different state-of-the-art approaches and give a summary of the results from their evaluation.
 

\section{The Winograd Schema Challenge}
\label{section:TheWinogradSchemaChallenge}

The WSC was originally conceived in 2012 \cite{DBLP:conf/kr/LevesqueDM12} and since then it has caught the  attention of many researchers from different areas. However, the first and so far only WSC competition was held in 2016 as part of IJCAI\footnote{http://ijcai-16.org/}. Before the competition, the rules for executing and evaluating the participants were presented by Morgenstern et al. \cite{DBLP:journals/aim/MorgensternDO16}. The competition consisted of two rounds, one qualifying and one final, each with 60 questions. For the qualifying round, the questions were randomly chosen from the PDPs dataset and for the final round the questions were from the WSC dataset. In the qualifying round none of the participants achieved accuracy of 90\%, which was the requirement for advancing to the final round. 


\subsection{Description}
A Winograd Schema (WS) consists of three main parts:

\begin{enumerate}
	\item A sentence that consists of:
	\begin{itemize}
		\item Two noun phrases of the same semantic class and of the same gender
		\item One ambiguous pronoun that could refer to either of the antecedent noun phrases
		\item A special word such that when changed, the resolution of the pronoun is changed
	\end{itemize}
    \item A question, possibly containing the special word, asking about the referent \\of the ambiguous pronoun
    \item Two possible answers corresponding to the noun phrases in the sentence
\end{enumerate}

The WSC dataset currently consists of 285\footnote{https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.xml} such WSs.
A typical example of a WS is the following one:
\labeltext{\textit{Example 2.1.1}}{Ex1}:
\begin{itemize} 
	\item[\textbf{S:}] \textbf{\underline{The trophy} does not fit into \underline{the brown suitcase} because \underline{it} is too small.}
	\item[\textbf{Q:}] \textbf{What is too small?}
	\item[\textbf{A:}] \textbf{\underline{The suitcase}/the trophy}
\end{itemize}

Here, the special word is the adjective at the end of the sentence which can be \textit{small} or \textit{big}. The ambiguous pronoun is \textit{it} and the two antecedents are \textit{trophy} and \textit{suitcase}. The adjective in the question depends on the chosen special word in the sentence. While it is obvious that answering the question in this example is easy for an adult English speaker, it still requires thinking to derive the correct answer. In order to identify the correct referent of \textit{it}, one needs to use world knowledge about the size of objects, more specifically that a small object can fit into a large one, but not the other way around. Hence, if a machine is able to resolve the pronoun correctly, we can conclude that background knowledge was involved. Answering the question in this example is known as a problem of pronoun disambiguation. However, the sentences in the WSs are distinct in that one special word, which when changed causes the other noun phrase to be the correct referent. For example, consider the twin sentence for the WS given in \ref{Ex1}:
%\textit{Example 2.1.2:}
\begin{itemize} 
	\item[\textbf{S:}] \textbf{\underline{The trophy} does not fit into the brown suitcase because \underline{it} is too large.}
	\item[\textbf{Q:}] \textbf{What is too large?}
\end{itemize}

The possible answers are the same as given in \ref{Ex1}, but here the correct answer is \textit{the trophy}. Having the special word in the sentences prevents the solver to rely on sentence structure and word order for finding the answer to the question. 
Moreover, Levesque et al. \cite{DBLP:conf/kr/LevesqueDM12} explain another feature of the WS sentences, which they call \textit{Google proof}: The idea is to prevent finding the answer by using statistical learning on large corpora of text or by just typing the question and the answers into a search engine, such as Google. 
Here is an example of a WS that is not Google proof:\\
\labeltext{\textit{Example 2.1.2}}{ex:Google}:

\begin{itemize} 
	\item[\textbf{S:}] \textbf{Tom's \underline{books} are full of \underline{mistakes}. Some of them are quite \underline{[foolish/worthless]}.}
	\item[\textbf{Q:}] \textbf{What are [foolish/worthless]?}
	\item[\textbf{A:}] \textbf{The mistakes/the books.} 
\end{itemize} 

\ref{ex:Google} has been provided as a failed example\footnote{https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSfailed.html} because the term \textit{foolish mistake} is commonly used and can be returned as an answer when querying Google. Additional feature is that resolving a WS should be easy  for a human reader. For this reason, human performance is expected to be near 100\%. Indeed, Bender \cite{DBLP:conf/maics/Bender15} conducted a large online experiment and reported 92\% success rate for humans. 
For the execution and evaluation part of the WSC, two distinct datasets have been formulated and published by Morgenstern et al. \cite{DBLP:journals/aim/MorgensternDO16}.
The first dataset consists of pronoun disambiguation problems (PDPs)\footnote{https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/PDPChallenge2016.xml} that are taken from examples found in literature, biographies, essays and news or have been constructed by the organizers of the competition. 
The second dataset consists of WSs\footnote{https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.html} which have been constructed by the challenge organizers.
The following is an example of the PDPs:\\
\labeltext{\textit{Example 2.1.3}}{PDP}:

\begin{itemize}[align=left] 
	\item [\textbf{Text:}] \textbf{When they had eventually calmed down a bit, and had gotten home, Mr. Farley put the \underline{magic pebble} in an iron \underline{safe}. Some day they might want \underline{to use it}, but really for now, what more could they wish for?}
	
	\item [\textbf{Snippet:}] \textbf{to use it}
	\item [\textbf{Answers:}] \textbf{\underline{magic pebble}/safe}

\end{itemize}

Along with the text, a snippet with the ambiguous pronoun that needs to be resolved and the possible answers are provided.
As in \ref{PDP}, a PDP may consist of more than one sentence and it can also have more than two possible answers. Similar to resolving WSs, considerable use of commonsense reasoning is required when answering the PDPs. Since the structure of the PDPs is not always consistent, resolving these problems is more difficult than resolving WSs.

Having the two different sets allows the evaluation of the participating systems in the challenge to be done gradually. 
If a system successfully pases the questions from the first dataset, then it will be challenged with the WSs set. Morgensten et al. \cite{DBLP:journals/aim/MorgensternDO16} argue that if a system can answer the problems from the PDP set correctly, then it will surely have success when answering the WSs set.  
Additionally, the PDPs are taken from various literature sources and may include different aspects of commonsense knowledge which would otherwise not be included in the created WSs. 

\begin{comment}
Since the idea of the authors of the WSC was to construct this challenge as an alternative to the Turing test, it can be observed that indeed it captures the main characteristics of a Turing test: 

\begin{itemize}
	\item it requires the subject to respond on a non-domain specific set of English sentences
	\item it is easy to solve by native English speakers
	\item to pass successfully the test, the subject has to think
\end{itemize}
\end{comment}


\subsection{Main challenges and limitations}
We now analyze and discuss the main challenges that an approach for addressing the WSC must overcome in order to achieve good results. Furthermore, we will identify and discuss the main limitations of the WSC. 

Finding the correct noun phrase for an ambiguous pronoun is a known problem in the Natural Language Processing (NLP) research field. Coreference resolution problem depends on different features, such as grammatical role, number, gender, syntactic structure and the distance between the referent and the pronoun. Although existing automatic systems rely on such grammatical features, they do not use world knowledge in the resolution process. Consequently, the Stanford resolver CoreNLP \cite{lee11conllst}  achieved accuracy of around 55\% on the dataset provided by Rahman and Ng \cite{DBLP:conf/emnlp/RahmanN12}, which is just above the random baseline (50\%).
	
From the examples given earlier, it is clear that background knowledge is required in order for a human reader to derive the correct answer. Since the sentences from both datasets, PDPs and WSs, are non-domain specific it is difficult to determine what kind of background knowledge is needed. Therefore, the first challenge imposed by the WSC is how to obtain knowledge for commonsense reasoning. 
The second is how to formalize this knowledge such that all important information are preserved. Lastly, how to reason on top of formalized knowledge is the third main challenge when trying to solve the WSC. 

Regarding the limitations of the WSC, as a first and probably most important limitation is the number of available sentences. Because the process of creating new WS is difficult, the number of available sentences is very small. Moreover no training data is provided for any of the datasets which makes it hard to approach the task as a machine learning problem. Another limitation is the constraint posed by the language of the sentences. For some other languages, as explained by Davis \cite{DBLP:journals/corr/Davis16}, the disambiguation of a pronoun for different genders may not be as clear as in English. Nevertheless, there are translation of 144 WSs in Japanese\footnote{https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/collection\_ja.pdf} and 107 WSs in French \footnote{http://www.llf.cnrs.fr/winograd-fr}. 

A third limitation is the lack of a more detailed evaluation protocol. In the proposal for the WSC competition the evaluation is measuring the accuracy of predicting the correct answers from both the PDPs and WSs datasets. However, Trichelair et al. \cite{DBLP:journals/corr/abs-1811-01778} argue that this evaluation does not measure whether commonsense reasoning was involved in the prediction of the correct answer. For this reason, they propose a different evaluation protocol which additionally includes measures of the accuracy on a changed subset and a consistency score. The changed subset contains WSs with switched antecedents such as in the following example. \\
\labeltext{\textit{Example 2.1.4}}{changed}:
\begin{itemize} 
	\item \textbf{S: \underline{Emma} did not pass the ball to \underline{Janie} although she saw that she was open.}
		\item \textbf{S': \underline{Janie} did not pass the ball to \underline{Emma} although she saw that she was open.} 
\end{itemize}

The consistency score is the percentage of predicted correct answer after the antecedents have changed. This new evaluation protocol gives a better understanding on how the models perform and opportunity to analyze in more details the achieved results.

 


